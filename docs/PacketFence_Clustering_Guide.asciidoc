Clustering Quick Installation Guide
===================================
:encoding: UTF-8
:lang: en
:doctype: book
:toc: left

include::includes/global-attributes.asciidoc[]

About this Guide
----------------
This guide has been created to give a quick start to install active/active clustering in PacketFence 7+. This guide does not include advanced troubleshooting of the active/active clustering. Refer to the documentation of HAProxy and Keepalive for advanced features.

Assumptions
-----------
* You have at least three (3) installed PacketFence (v7+) servers
* Servers are running RHEL / CentOS 7
* Servers have identical copies for network interfaces
* Servers network interfaces are on the same layer 2 network
* Servers hostnames must be resolvable via DNS resolution

NOTE: Appended to this guide is a glossary on specialized terms used in this document.

Installation on CentOS 7
------------------------

Step 1: Install the replicated database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NOTE: In this example, the database stack uses the native PacketFence MariaDB Galera cluster integration. Although other MySQL based clustering stacks are supported, they aren't covered in this guide. If you use an external database or want to use another clustering stack for the database, you can ignore this section and jump to Step 2 directly.

CAUTION: Galera cluster is only supported in 3 nodes cluster and more (with an odd number of servers).

PacketFence ships with a version of MariaDB that has Galera cluster built-in, so no additionnal packages are required.

Configuring Galera
^^^^^^^^^^^^^^^^^^

For the next steps, you want to make sure that you didn't configure anything in `/usr/local/pf/conf/cluster.conf`. If you already did, comment all the configuration in the file and do a configreload (`/usr/local/pf/bin/pfcmd configreload hard`).

On each server of your cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

First, start packetfence-mariadb and make sure it was able to start in 'standalone' mode.

  # systemctl start packetfence-mariadb

Then, secure your installation

  # mysql_secure_installation

Then, you will need to create a user for the database replication that PacketFence will use. You can use any username/password combination. After creating the user, keep its informations close-by for usage in the configuration.

  mysql -u root -p

    mysql> CREATE USER 'pfcluster'@'%' IDENTIFIED BY 'aMuchMoreSecurePassword';
    mysql> GRANT RELOAD, LOCK TABLES, REPLICATION CLIENT ON *.* TO 'pfcluster'@'%';

    mysql> CREATE USER 'pfcluster'@'localhost' IDENTIFIED BY 'aMuchMoreSecurePassword';
    mysql> GRANT RELOAD, LOCK TABLES, REPLICATION CLIENT ON *.* TO 'pfcluster'@'localhost';

    mysql> FLUSH PRIVILEGES;

Next, you need to remove the empty users from your MySQL database:

  # mysql -u root -p
    mysql> delete from mysql.user where user = '' ;
    mysql> flush privileges;

Step 2 : Server configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

First, you need to make sure the interfaces name will be the same on both servers. See 'Setting the interfaces name on CentOS 7' in the Appendix section of this document if all your servers don't already have the same interface names.

Next, you will need to configure the server so the services can bind on IP addresses they don't currently have configured. This allows faster failover of the services.

On all your servers, add the following line in `/etc/sysctl.conf` and then reload with 'sysctl -p'

  net.ipv4.ip_nonlocal_bind = 1

Now, on the first server, create the PEM that combines the key and certificate for the http services. Adapt to your own paths if you are using different certificates. Most important is that the destination PEM file is `/usr/local/pf/conf/ssl/server.pem` as its the one used in haproxy.

  # cd /usr/local/pf/conf/ssl
  # cat server.key server.crt > server.pem

PacketFence Configuration Modification
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In order for PacketFence to communicate properly with your MySQL cluster, you need to change the following.
This change only needs to be done on the first server of the cluster. It will be synchronized later.

In `conf/pf.conf` :

----
[database]
host=127.0.0.1

[monitoring]
db_host=127.0.0.1

[active_active]
# Change these 2 values by the credentials you've set when configuring MariaDB above
galera_replication_username=pfcluster
galera_replication_password=aMuchMoreSecurePassword
----

In `conf/pfconfig.conf` :

----
[mysql]
host=127.0.0.1
----

Now, reload the configuration and restart packetfence-config. You will see errors related to a cache write issue but you can safely ignore it for now. These appear because packetfence-config cannot connect to the database yet.

  # systemctl restart packetfence-config
  # /usr/local/pf/bin/pfcmd configreload hard

Going through the configurator
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Now, on the first server of your cluster, you should go through the configurator, configuring the server like if it was in standalone (no HA interfaces). You should leave the services stopped at the end of the configurator.

On the other servers of your cluster, configure only the network interfaces without going past that section in the configurator.

Step 3 : Create a new cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to create a new cluster, you need to configure `/usr/local/pf/conf/cluster.conf`

You will need to configure it with your server hostname. To get it use : *hostname* in a command line.

In the case of this example it will be 'pf1.example.com'. 

The 'CLUSTER' section represents the virtual IP addresses of your cluster that will be shared by your servers.

In this example, eth0 is the management interface, eth1.2 is the registration interface and eth1.3 is the isolation interface.

On the first server, create a configuration similar to this : 

----

[CLUSTER]
management_ip=192.168.1.10

[CLUSTER interface eth0]
ip=192.168.1.10
type=management,high-availability

[CLUSTER interface eth1.2]
ip=192.168.2.10
type=internal

[CLUSTER interface eth1.3]
ip=192.168.3.10
type=internal

----
----

[pf1.example.com]
management_ip=192.168.1.5

[pf1.example.com interface eth0]
ip=192.168.1.5
type=management,high-availability
mask=255.255.255.0

[pf1.example.com interface eth1.2]
enforcement=vlan
ip=192.168.2.5
type=internal
mask=255.255.255.0

[pf1.example.com interface eth1.3]
enforcement=vlan
ip=192.168.3.5
type=internal
mask=255.255.255.0

----
----

[pf2.example.com]
management_ip=192.168.1.6

[pf2.example.com interface eth0]
ip=192.168.1.6
type=management,high-availability
mask=255.255.255.0

[pf2.example.com interface eth1.2]
enforcement=vlan
ip=192.168.2.6
type=internal
mask=255.255.255.0

[pf2.example.com interface eth1.3]
enforcement=vlan
ip=192.168.3.6
type=internal
mask=255.255.255.0

----
----

[pf3.example.com]
management_ip=192.168.1.7

[pf3.example.com interface eth0]
ip=192.168.1.7
type=management,high-availability
mask=255.255.255.0

[pf3.example.com interface eth1.2]
enforcement=vlan
ip=192.168.2.7
type=internal
mask=255.255.255.0

[pf3.example.com interface eth1.3]
enforcement=vlan
ip=192.168.3.7
type=internal
mask=255.255.255.0

----
Once this configuration is done, reload the configuration and restart PacketFence to have it applied:
  
 # /usr/local/pf/bin/pfcmd configreload hard
 # /usr/local/pf/bin/pfcmd checkup
 # /usr/local/pf/bin/pfcmd service pf restart

Then make sure the packetfence clustering services will be started at boot by running the following command on all of your servers

 # systemctl set-default packetfence-cluster

You should now restart packetfence-mariadb on the first server so that it now starts in clustered mode. Note that it will fail to start properly until at least one of the other node is alive (pingable). Once an alive quorum is reached it will be able to create a new cluster.

 # systemctl restart packetfence-mariadb

If no error is found in the previous configuration, the previous restart of packetfence should have started keepalived and radiusd-loadbalancer along with the other services.

You should now have service using the first server on the IP addresses defined in the 'CLUSTER' sections.

Step 4: Integrating the two other nodes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now, you will need to integrate your 2 other nodes in your cluster.

Get the webservices user and password on the master node in 'Configuration/Web Services'
If there's none, set the user, password and then restart httpd.webservices on the first server.

Do (and make sure it does it without any errors) :

  # /usr/local/pf/bin/cluster/sync --from=192.168.1.5 --api-user=packet --api-password=fence

Where : 

* '192.168.1.5' is the management IP of the other PacketFence node
* 'packet' is the webservices username you have setup on the master node
* 'fence' is the webservices password you have setup on the master node

Reload the configuration and start the webservices on the new server

  # service packetfence-config restart
  # /usr/local/pf/bin/pfcmd configreload
  # /usr/local/pf/bin/pfcmd service haproxy restart
  # /usr/local/pf/bin/pfcmd service httpd.webservices restart

Make sure that this server is binding to it's own management address. If it's not, verify the `/usr/local/pf/conf/cluster.conf` management interface configuration.

  # netstat -nlp | grep 9090

Now replicate this server configuration to the other nodes in the cluster

  # /usr/local/pf/bin/cluster/sync --as-master

Make sure at least /usr/local/pf/conf/cluster.conf was replicated to the other servers

Make sure to join domains through 'Configuration/Domains' on each additional create node.

Now restart packetfence on each cluster server keeping the new node as the last one to be restarted.

Step 5: Securing the cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Keepalived secret
^^^^^^^^^^^^^^^^^

It is highly recommended to modify the keepalive shared secret in your cluster to prevent attacks. In the administration interface, go in 'Configuration/Clustering' and change the 'Shared KEY'. Make sure you restart keepalived on both your servers using `/usr/local/pf/bin/pfcmd service keepalived restart`

Understanding the Galera cluster synchronization
------------------------------------------------

The Galera cluster stack used by PacketFence ressembles a lot to how a normal MariaDB Galera cluster behaves but it contains hooks to auto-correct some issues that can occur.

NOTE: A lot of useful information is logged in the MariaDB log which can be found in `/var/lib/mysql/PF_HOSTNAME.err`

Quorum behavior
~~~~~~~~~~~~~~~

A loss of quorum is when a server is not able to be part of a group that represents more than 50% of the configured servers in the cluster. This can occur if a node is isolated from a network perspective or if more than 50% of its peers aren't alive (like in the case of a power outage).

The Galera cluster stack will check in permanence that it has a quorum. Should one of the server be part of a group that doesn't have the quorum in the cluster, it will put itself in read-only mode and stop the synchronization. During that time, your PacketFence installation will continue working but with some features disabled.

 * RADIUS Mac Authentication: Will continue working and will return RADIUS attributes associated with the role that is registered in the database. If VLAN or RADIUS filters can apply to this device, they will be but any role change will not be persisted.
 * RADIUS 802.1x: Will continue working and if 'Dot1x recompute role from portal' is enabled, it will compute the role using the available authentication sources but will not save it in the database at the end of the request. If this parameter is disabled, it will behave like MAC Authentication. VLAN and RADIUS filters will still apply for the connections. If any of your sources are external (LDAP, AD, RADIUS, ...), they must be available for the request to complete successfully.
 * Captive portal: The captive portal will be disabled and display a message stating the system is currently experiencing an issue.
 * DHCP listeners: The DHCP listeners will be disabled and packets will not be saved in the database. This also means Firewall SSO will not work during that time.

Once the server that is in read-only mode joins a quorum, it will go back in read-write mode and the system will go back to its normal behavior automatically.

Graceful shutdown behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~

When you are gracefully shutting down servers for a planned maintenance, you should always aim to leave a quorum alive so that once the server joins its peers again, it will always re-join the cluster gracefully. You can also leave only one of the nodes alive but keep in mind it will fall in read-only mode but once another node joins the cluster, it will be able to join gracefully.

Should all your nodes be shutdown gracefully (not force killed), when they come back online, they will not be able to create a quorum automatically using the built-in Galera cluster stack. In this case, if the management node will self-elect as a master and have the other nodes join the cluster. This means that if you gracefully shutdown the management node 30 minutes before shutting down the other nodes, when they come back online you will lose the 30 minutes of data that aren't on the management node. 

Note that the self-election process of the management node can take up to 5 minutes and requires that more than 50% of the nodes be alive (pingable) during that time.

Knowing this, you should never restart all of your nodes at the same time and should always aim to leave at least one available and online so that it knows about the previous state of the cluster. When possible, leave a quorum alive.

You can prevent the management node from self-electing as master by putting it in maintenance mode before you shut it down (See 'Putting a node in maintenance'). When you do so, be aware that if the other 2 nodes shutdown, there will not be any self-election and the cluster will be waiting for a manual intervention from an administrator to start. You can force any node to become a master and create a new cluster using `/usr/local/pf/bin/mariadb --force-new-cluster`. Then wait until another server connects to this new cluster, break the command and start packetfence-mariadb like you normally would.

Ungraceful shutdown behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NOTE: You can know a node was hard-shutdown if `/var/lib/mysql/gvwstate.dat` exists on the node.

All nodes shutdown at the same time
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If all of your nodes experience a hard-shutdown or hard-reset (like in the case of a power outage), they will all reconnect gracefully when *all* nodes of the cluster come back up. If only one of the node that was alive at the time of the outage is missing, the cluster will wait for this node to come back and no service will be given during that time. 

Recovering when a node is missing
+++++++++++++++++++++++++++++++++

If one of the nodes cannot recover, you can manually reset the cluster state. Note that this procedure is only valid if all the nodes were hard-shutdown at the same time.

First, stop packetfence-mariadb on all servers:

  # systemctl stop packetfence-mariadb

Then remove `/var/lib/mysql/gvwstate.dat` on all the servers that are still alive.

Then, if the management node is part of the alive nodes, start all of your servers and the management node will make sure to establish a new cluster. Note that this process can take up to 5 minutes to occur and will only occur if more than 50% of the nodes are alive (pingable).

If the management node is not part of the alive nodes, select one of the servers of your cluster and create a new cluster using `/usr/local/pf/bin/mariadb --force-new-cluster`. Wait for another server to connect to this new cluster, break the command and start packetfence-mariadb like you normally would.

All nodes shutdown with an interval
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Should all nodes be shutdown with an interval (even if it is in seconds), they will not be able to reconnect automatically using the Galera cluster stack.

The cluster will attempt to automatically recover using this strategy:

'TBD'

Putting a node in maintenance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When doing maintenance on a node (especially the management node), it is always better to put it in maintenance mode so it doesn't try to join an existing cluster.

In order to activate the maintenance mode on a node:

  # /usr/local/pf/bin/cluster/maintenance --activate

In order to deactivate the maintenance mode on a node:

  # /usr/local/pf/bin/cluster/maintenance --deactivate

In order to see the current maintenance state on a node:

  # /usr/local/pf/bin/cluster/maintenance

Advanced configuration
----------------------

Removing a server from the cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NOTE: Removing a server from the cluster requires a restart of the PacketFence service on all nodes.

First, you will need to stop PacketFence on your server and put it offline.

  # service packetfence stop
  # shutdown -h 0

Then you need to remove all the configuration associated to the server from /usr/local/pf/conf/cluster.conf on one of the remaining nodes. 
Configuration for a server is always prefixed by the server's hostname.

Once you have removed the configuration, you need to reload it and synchronize it with the remaining nodes in the cluster.

  # /usr/local/pf/bin/pfcmd configreload hard
  # /usr/local/pf/bin/cluster/sync --as-master

Now restart PacketFence on all the servers so that the removed node is not part of the clustering configuration.

Note that if you remove a node and end up having an even number of servers, you will get unexpected behaviors in MariaDB. You should always aim to have an odd number of servers at all time in your cluster.

Resynchronizing the configuration manually
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you did a manual change in a configuration file, an additional step is now needed.

In order to be sure the configuration is properly synced on all nodes, you will need to enter this command on the previously selected master node.

----
# /usr/local/pf/bin/cluster/sync --as-master
----

Adding files to the synchronization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the event that you do modifications to non-synchronized files like switch modules, files in raddb/, etc, you can add those files to be synchronized when using /usr/local/pf/bin/cluster/sync.

On one of the nodes, create /usr/local/pf/conf/cluster-files.txt

Add the additional files one per line in this file. We advise you add this file to the synchronization too.

Example : 

----
/usr/local/pf/conf/cluster-files.txt
/usr/local/pf/raddb/modules/mschap
----

haproxy dashboard
~~~~~~~~~~~~~~~~~

You have the possibility to configure the haproxy dashboard which will give you statistics about the current state of your cluster.

In order to active it uncomment the following lines from /usr/local/pf/conf/haproxy.conf

----
listen stats %%active_active_ip%%:1025
  mode http
  timeout connect 10s
  timeout client 1m
  timeout server 1m
  stats enable
  stats uri /stats
  stats realm HAProxy\ Statistics
  stats auth admin:packetfence
----

NOTE: We strongly advise you change the username and password to something else than admin/packetfence although access to this dashboard doesn't compromise the server.

Next, uncomment the following line from /usr/local/pf/conf/iptables.conf

CAUTION: If you're upgrading from a version prior to 5.0, the line may not be there. Add it close to the other management rules

----
-A input-management-if --protocol tcp --match tcp --dport 1025 --jump ACCEPT
----

Now restart haproxy and iptables in order to complete the configuration

----
# /usr/local/pf/bin/pfcmd service haproxy restart
# /usr/local/pf/bin/pfcmd service iptables restart
----

You should now be able to connect to the dashboard on the following URL : http://pf.local:1025/stats

Configuration conflict handling
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NOTE: It is not recommended to perform configuration while one or more node of the cluster is experiencing issues. Still, should that be the case, this section will explain the conflict resolution that will occur when the nodes will reattach together.

When modifying the configuration through the administration interface, the configuration will be automatically synchronized to all the nodes that are online. In the event that one or more nodes cannot be updated, an error message will be displayed with affected nodes.

A scheduled check runs on the management server (controlled through maintenance.cluster_check_interval) in order to validate if all servers are running the same configuration version.
When the failed node(s) will come back online, that scheduled check will ensure that the new configuration is pushed on the new node(s).
You can disable this check by setting maintenance.cluster_check_interval to 0 and restarting pfmon. In that case, you will need to manually resolve the conflict when the node(s) come back online by running `/usr/local/pf/bin/cluster/sync --as-master` on the node you want to keep the configuration of.

General facts about conflict resolution:

 * If the configuration is not pushed to at least half of the servers of your cluster, when the failed nodes will come back online, they will have quorum on the previous configuration and the one they are running will be pushed to all the servers.
 * In a two node cluster, the most recent configuration is always selected when resolving a conflict.
 * In a two node cluster, no decision is taken unless the peer server has its webservices available.

Going deeper in the conflict handling
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The section below will explain with more details, the steps that are taken in order to take the decision of which server should be declared as the master when one or more servers have conflicting configuration version.

The first step is to get the configuration version from each server through a webservice call.

The results are then organized by version identifier. Should all alive servers run the same version, the state is considered as healthy and nothing happens.

Then, should there be more than one version identifier across the alive servers, the algorithm validates that there are at least 2 servers **configured** in the cluster. If there aren't, then the most recent version is pushed on the peer node.

After that, the algorithm looks at which version is on the most servers. In the event that the dead servers are in higher number than the alive ones, the most recent version is taken. Otherwise, the version that is present on the most servers will be selected.

When pushing a version to the other servers, if the current version has the most recent version or is part of the majority (depending on which push strategy was defined above), then he will be the one pushing the new configuration to the other servers. Otherwise, a webservice call is made to one of the servers running the selected version so that he pushes its configuration to its peers.

Unsupported features in active/active
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following features are not supported when using active/active clustering.

* Switches using SNMP based enforcement (port-security, link up/down, ...)
* SNMP roaming with the Aerohive controller. (4.7+ includes support for accounting roaming)
* Inline enforcement

Appendix
--------

Glossary
~~~~~~~~

 * 'Alive quorum': An alive quorum is when more than 50% of the servers of the cluster are online and reachable on the network (pingable). This doesn't imply they offer service, but only that they are online on the network.
 * 'Hard-shutdown': A hard shutdown is when a node or a service is stopped without being able to go through a proper exit cleanup. This can occur in the case of a power outage, hard reset of a server or `kill -9` of a service.
 * 'Management node/server': The first server of a PacketFence cluster as defined in `/usr/local/pf/conf/cluster.conf`.
 * 'Node': In the context of this document, a node is a member of the cluster while in other PacketFence documents it may represent an endpoint.

Setting the interfaces name on CentOS 7
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On CentOS 7 you need to make sure that all the servers in the cluster use the same interfaces name.
This section covers how to set the interfaces to the ethX format. 
Note that you can set it to the format you desire as long as the names are the same on all servers.

First, go in `/etc/default/grub` and add `net.ifnames=0` to the variable: `GRUB_CMDLINE_LINUX`.

  GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet net.ifnames=0"

Then regenerate the GRUB configuration by executing the following command:

  # grub2-mkconfig -o /boot/grub2/grub.cfg

Then, rename the network script of your management interface (`eno16780032` in this example) to be in the ethX form (`eth0` in this example)

  # mv /etc/sysconfig/network-scripts/ifcfg-eno16780032 /etc/sysconfig/network-scripts/ifcfg-eth0

And rename the name of the interface in the following files (making sure you replace `eno16780032` and `eth0` by the appropriate values):

  # sed -i.bak "s/eno16780032/eth0/g" /etc/sysconfig/network-scripts/ifcfg-eth0
  # sed -i.bak "s/eno16780032/eth0/g" /usr/local/pf/conf/pf.conf

Apply the last two steps for any other interface you have on your server. Keep in mind, you can use the PacketFence configurator to reconfigure them later as long as your management interface is correctly configured and is accessible on your network.

Now, reboot your server and when it finishes starting, your interfaces name should now be using the format `ethX`

